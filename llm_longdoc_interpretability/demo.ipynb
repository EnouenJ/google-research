{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_EEHLUogjT_"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0\n",
        "\n",
        "# inference.py\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import T5Model, T5ForConditionalGeneration\n",
        "import os\n",
        "\n",
        "SUPPORTED_MODEL_DICT = {\n",
        "  'gemma7b' : \"google/gemma-7b\",\n",
        "  'gemma2b' : \"google/gemma-2b\",\n",
        "  'llama3'  : \"meta-llama/Meta-Llama-3-8B\",\n",
        "  't5-large' : \"google-t5/t5-large\",\n",
        "}\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "class InferencePlatform(ABC):\n",
        "  \"\"\"An abstract class for the LLM inference platform we use.\"\"\"\n",
        "  @abstractmethod\n",
        "  def predict(self, prompt: str) -> str:\n",
        "    pass\n",
        "\n",
        "class HuggingFace(InferencePlatform):\n",
        "  \"\"\"An implementation for using HuggingFace as the platform for LLM inference.\"\"\"\n",
        "  def __init__(self):\n",
        "    self._tokenizer = None\n",
        "    self._model = None\n",
        "\n",
        "  def authenticate(self, huggingface_token):\n",
        "    os.environ['HF_TOKEN'] = huggingface_token\n",
        "\n",
        "  def setup_model(self, model_name: str):\n",
        "    if model_name not in SUPPORTED_MODEL_DICT:\n",
        "      raise ValueError(f'Unsupported model: {model_name}')\n",
        "    self.model_name=model_name\n",
        "    if model_name in ['gemma2b','gemma7b','llama3']:\n",
        "      hf_path = SUPPORTED_MODEL_DICT[model_name]\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
        "      self.model = AutoModelForCausalLM.from_pretrained(hf_path)\n",
        "\n",
        "      # self.tokenizer = AutoTokenizer.from_pretrained(\"gdrive/My Drive/Colab Notebooks/gemma-7b-tokenizer\", local_files_only=True)\n",
        "      # self.model = AutoModelForCausalLM.from_pretrained(\"gdrive/My Drive/Colab Notebooks/gemma-7b-model\", local_files_only=True)\n",
        "    else:\n",
        "      hf_path = SUPPORTED_MODEL_DICT[model_name]\n",
        "      self.tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
        "      self.model = T5ForConditionalGeneration.from_pretrained(hf_path)\n",
        "\n",
        "\n",
        "  def predict(self, prompt: str) -> str:\n",
        "    inputs = self.tokenizer(prompt, return_tensors='pt')\n",
        "    generate_ids = self.model.generate(inputs.input_ids)\n",
        "    return self.tokenizer.batch_decode(generate_ids, skip_special_tokens=True)\n",
        "\n",
        "class VertexAI(InferencePlatform):\n",
        "  \"\"\"An implementation for using Google Cloud's Vertex AI as the platform for\n",
        "  LLM inference.\"\"\"\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def predict(self, prompt: str) -> str:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCd-PrA4jhFB"
      },
      "outputs": [],
      "source": [
        "# tree_utils.py\n",
        "\n",
        "def build_tree(dataset_path):\n",
        "  \"\"\"Builds and returns the hierarchical document tree.\"\"\"\n",
        "  pass\n",
        "\n",
        "def search_tree(doc_tree, inference_platform: InferencePlatform):\n",
        "  \"\"\"Searches the hierarchical document tree recursively.\"\"\"\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBYpaeRzloJe"
      },
      "outputs": [],
      "source": [
        "# run.py\n",
        "\n",
        "HF_TOKEN = \"\" # add your hugging face access token here\n",
        "\n",
        "\n",
        "def main():\n",
        "  inference_platform = HuggingFace()\n",
        "  inference_platform.authenticate(HF_TOKEN)\n",
        "  inference_platform.setup_model('gemma2b')\n",
        "\n",
        "  test_prediction=inference_platform.predict(\"question: Who earned the first nobel prize in physics? \\n answer:\")\n",
        "  print(test_prediction)\n",
        "\n",
        "  dataset_path = \"\"\n",
        "  # Build hierarchical document tree.\n",
        "  doc_tree = build_tree(dataset_path)\n",
        "\n",
        "  # Search tree recursively.\n",
        "  search_tree(doc_tree, inference_platform)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlW4G2Imn7LB"
      },
      "outputs": [],
      "source": [
        "# visualize.py\n",
        "\n",
        "# TODO(james): See if we can reuse the original visualization from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLOaVp_KI9Q9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlOGUCsqV9u7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
